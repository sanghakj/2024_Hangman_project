{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Submitted by Sanghak Jeon\n",
    "#### Submitted on 02/19/2024\n",
    "<br>\n",
    "\n",
    "#### Contacts\n",
    "Email         : sanghakj@princeton.edu\n",
    "Mobile        : 609-558-6291 \n",
    "Linkedin      : <a href = https://www.linkedin.com/in/sanghak-jeon/>https://www.linkedin.com/in/sanghak-jeon/</a>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3097c50740a46ea6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contents\n",
    "\n",
    "* Dataset analysis\n",
    "* Logistics\n",
    "    - n-gram\n",
    "    - minor details\n",
    "* Trials and Ideas\n",
    "* Result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b41d6b9e7ad9a12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Dataset analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "24faf0cd9b321e0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The file \"words_250000_train.txt\" was given, so I tried to check possible glitches/data corruptions. There was none.\n",
    "\n",
    "* There are exactly 227300 words.\n",
    "* The maximum length of the word from \"words_250000_train.txt\" is 29.\n",
    "* The distribution by word's length is\n",
    "    3: 2201, 6: 19541, 4: 5287, 5: 11274, 8: 30452,\n",
    "    7: 25948, 10: 26953, 9: 30906, 11: 22786, 12: 18178,\n",
    "    13: 12956, 15: 5211, 14: 8710, 20: 225, 17: 1775,\n",
    "    16: 3143, 2: 264, 21: 98, 18: 859, 19: 441,\n",
    "    25: 3, 22: 44, 1: 17, 23: 14, 29: 2,\n",
    "    24: 9, 28: 1, 27: 2\n",
    "    (these are unsorted, because I calculated with a dictionary structure.)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbd9193d5cbc9f49"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {3: 2201, 6: 19541, 4: 5287, 5: 11274, 8: 30452, 7: 25948, 10: 26953, 9: 30906, 11: 22786, 12: 18178, 13: 12956, 15: 5211, 14: 8710, 20: 225, 17: 1775, 16: 3143, 2: 264, 21: 98, 18: 859, 19: 441, 25: 3, 22: 44, 1: 17, 23: 14, 29: 2, 24: 9, 28: 1, 27: 2})\n"
     ]
    }
   ],
   "source": [
    "import time, collections\n",
    "\n",
    "alphabets ={'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}\n",
    "vowels = {'a', 'e', 'i', 'o', 'u'}\n",
    "semivowels = {'y'}                          # linguistically y sometimes behave as a vowel\n",
    "consonants = list(set(alphabets) - set(vowels) - set(semivowels))\n",
    "\n",
    "def build_dictionary(dictionary_file_location):\n",
    "    text_file = open(dictionary_file_location,\"r\")\n",
    "    full_dictionary = text_file.read().splitlines()\n",
    "    text_file.close()\n",
    "    return full_dictionary\n",
    "\n",
    "# dictionary loading\n",
    "full_dictionary_location = \"words_250000_train.txt\"\n",
    "full_dictionary = build_dictionary(full_dictionary_location)\n",
    "\n",
    "# dictionary length counting\n",
    "length = collections.defaultdict(int)\n",
    "for word in full_dictionary:\n",
    "    length[len(word)]+=1\n",
    "print(length)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5310eada7a3a3b00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Furthermore, I need some distributional information on consonants for later."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed952a8897c5790"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 4-grams with all consonants\u001B[39;00m\n\u001B[0;32m      2\u001B[0m totalcount, count, wordcount, iscccc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m full_dictionary:\n\u001B[0;32m      4\u001B[0m     iscccc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(word) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m:\n",
      "\u001B[1;31mNameError\u001B[0m: name 'full_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "# 4-grams with all consonants\n",
    "totalcount, count, wordcount, iscccc = 0, 0, 0, False\n",
    "for word in full_dictionary:\n",
    "    iscccc = False\n",
    "    if len(word) >= 4:\n",
    "        for i in range(len(word)-3):\n",
    "            totalcount += 1\n",
    "            if (word[i] in consonants) and (word[i+1] in consonants) and (word[i+2] in consonants) and (word[i+3] in consonants):\n",
    "                # print(f' sequence       \"{word[i:i+4]}\", from {word}')\n",
    "                count += 1\n",
    "                iscccc = True\n",
    "    wordcount += iscccc\n",
    "print(f' total {totalcount} number of 4-grams and total {count} number of 4-consonant-only-grams. total {wordcount} '\n",
    "      f'number of words including those.')\n",
    "\n",
    "\n",
    "# 5-grams with all consonants\n",
    "totalcount, count, wordcount, isccccc = 0, 0, 0, False\n",
    "for word in full_dictionary:\n",
    "    isccccc = False\n",
    "    if len(word) >= 5:\n",
    "        for i in range(len(word)-4):\n",
    "            totalcount += 1\n",
    "            if ((word[i] in consonants) and (word[i+1] in consonants) and (word[i+2] in consonants) and (word[i+3] in consonants) and (word[i+4] in consonants)):\n",
    "                # print(f' sequence       \"{word[i:i+5]}\", from {word}')\n",
    "                count += 1\n",
    "                isccccc = True\n",
    "    wordcount += isccccc\n",
    "print(f' total {totalcount} number of 5-grams and total {count} number of 5-consonant-only-grams. total {wordcount} '\n",
    "      f'number of words including those.')\n",
    "\n",
    "\n",
    "# 6-grams with all consonants\n",
    "totalcount, count, wordcount, iscccccc = 0, 0, 0, False\n",
    "for word in full_dictionary:\n",
    "    iscccccc = False\n",
    "    if len(word) >= 6:\n",
    "        for i in range(len(word)-5):\n",
    "            totalcount += 1\n",
    "            if ((word[i] in consonants) and (word[i+1] in consonants) and (word[i+2] in consonants) and (word[i+3] in consonants) and (word[i+4] in consonants) and (word[i+5] in consonants)):\n",
    "                # print(f' sequence       \"{word[i:i+6]}\", from {word}')\n",
    "                count += 1\n",
    "                iscccccc = True\n",
    "    wordcount += iscccccc\n",
    "print(f' total {totalcount} number of 6-grams and total {count} number of 6-consonant-only-grams. total {wordcount} '\n",
    "      f'number of words including those.')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T20:15:36.331192400Z",
     "start_time": "2024-09-10T20:15:35.901341500Z"
    }
   },
   "id": "f54e8bdfc4b33bcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Logistics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "145312445f959ade"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1. n-gram\n",
    "\n",
    "The most common approach (and also naive) is the n-gram (<a href = https://en.wikipedia.org/wiki/N-gram>https://en.wikipedia.org/wiki/N-gram</a>). In a real hangman \n",
    "game, often people choose 'e' for their first guess, not only 'e' is a vowel but also statistically 'e' comes out \n",
    "more frequently. n-gram idea is a generalization of frequency-based likelihood calculation. As an example of \n",
    "2-gram, we can calculate all frequencies over possible two consecutive letters, from \"aa\" to \"zz\". We can think a \n",
    "chunk of two letters such as \"ch\" or \"ll\" would be more frequent compare to \"qv\" or \"iw\". This idea can be \n",
    "generalized into more letters: 3-grams would detect phrases such as \"-ing\", \"pre-\", and \"-ght-\". 4-grams would detect\n",
    "phrases such as \"-tion\", \"-ness\", and \"auto-\". 5-grams would detect \"under-\", \"-ative\", \"-phone\".\n",
    "<br>\n",
    "\n",
    "Adapting n-gram algorithm brings two natural questions:\n",
    "* How many different n-grams do we need? Will 10-grams be helpful?\n",
    "* How are we going to 'interpolate/mix' results from different n-grams? How much should we consider more about \n",
    "2-gram results compare to 1-gram results?\n",
    "<br>\n",
    "\n",
    "Theoretically, there are 26^10 possible alphabet 10-grams. In order to count the frequency of 10-grams, the word must\n",
    " consist of more than 10 letters, and those collected 10-grams will be evenly distributed unless there is a common \n",
    " prefix/suffix consists of 10 or more letters. Hence the usefulness of n-grams must rely on how many common \n",
    " prefixes/suffixes are there with n or more letters. \n",
    "<br>\n",
    "\n",
    "I have checked these websites:\n",
    "* <a href = https://en.wikipedia.org/wiki/English_prefix>https://en.wikipedia.org/wiki/English_prefix</a>\n",
    "* <a href = https://dictionary.cambridge.org/us/grammar/british-grammar/prefixes>https://dictionary.cambridge\n",
    ".org/us/grammar/british-grammar/prefixes</a>\n",
    "\n",
    "I could investigate this further, but I decided to stop because this might violate external-material training rule. \n",
    "Also empirically I can imagine 5 letter prefix/suffix like \"extra-\", \"hyper-\", \"super-\", \"trans-\", \"-ology\", \n",
    "\"-ation\", \"-pathy\" while I cannot think of 6+ letter prefix/suffix other than \"contra-\". Hence I decided to only \n",
    "consider 1-gram to 5-gram.\n",
    "<br>\n",
    "\n",
    "For the second question, let's consider \"webs_te\". If there are enough \"-site-\"s on the original dictionary, \n",
    "guessing according to 4-gram would give a better prediction compare to guessing only by the frequency of each letters\n",
    "(1-gram). Hence we can think the weights for n-gram must be larger as n grows. Furthermore, as n-gram information \n",
    "becomes sparser as n grows(since there are $26^n$ possible configurations), the weights should be even larger. We can \n",
    "try to give this weight information as a function of (disclosed letters)/(total letters), (for \"webs_te\" it becomes \n",
    "6/7), but I decided not to. The weights on n-gram should grow exponentially on n (compare to $26^n$), so I decided to \n",
    "consider $[1, 2, 4, 8, 16]$ as a baseline."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa622c3a3673b6f3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2. code details \n",
    "\n",
    "Below is the code that I used for the final submission. Other than '__init__', 'guess', 'frequency_calc', \n",
    "'fivegram_adder' to 'unigram_adder', I intentionally substituted all contents to \"pass\". 'build_dictionary' method was \n",
    "premade and I didn't touch it, but I left it untouched because 'guess' calls it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc3d8c64c4765044"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "### hangman API\n",
    "\n",
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        \n",
    "        # training data importing        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        # information for guesses\n",
    "        self.current_dictionary = []\n",
    "        self.guessed_letters = []\n",
    "        self.alphabets = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', \n",
    "                          's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}\n",
    "        \n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        pass\n",
    "\n",
    "    def guess(self, word):\n",
    "        # word input example: \"a p p l e \" == \" _ p p _ e\"\n",
    "        # should return a letter\n",
    "        \n",
    "        clean_word = word[::2].replace(\"_\",\".\")     # clean_word: \".pp.e\"\n",
    "        \n",
    "        self.current_trial_count = len(self.guessed_letters)\n",
    "        self.wrong_guesses = list(set(self.guessed_letters) - set(clean_word))\n",
    "        self.correct_guesses = list(set(self.guessed_letters) - set(self.wrong_guesses))\n",
    "            # beware: from the example 'p' does not count twice\n",
    "        empty_sites = 0\n",
    "        for letter in clean_word:\n",
    "            empty_sites += (letter == \".\")\n",
    "        self.empty_sites = empty_sites\n",
    "                \n",
    "        # grab current dictionary of possible words from self object, initialize new possible words dictionary to empty\n",
    "        current_dictionary = self.current_dictionary\n",
    "        \n",
    "        if (len(self.guessed_letters) > 0 \n",
    "                and self.guessed_letters[-1] in self.wrong_guesses \n",
    "                and len(self.wrong_guesses) >= 3):\n",
    "            new_dictionary = []\n",
    "            for word in current_dictionary:\n",
    "                if not set(word).intersection(self.wrong_guesses):\n",
    "                    new_dictionary += [word]\n",
    "                  \n",
    "            # overwrite old possible words dictionary with updated version\n",
    "            self.current_dictionary = new_dictionary\n",
    "        \n",
    "        (self.unigram_frequency, self.twogram_frequency, self.threegram_frequency, self.fourgram_frequency, self\n",
    "        .fivegram_frequency) = (self.frequency_calc(self.current_dictionary))\n",
    "        self.weight = [1, 2, 4, 8, 16]\n",
    "            # sum should be 1; weight[0], weight[1], weight[2] corresponds to unigram/2-gram/3-gram\n",
    "        \n",
    "        self.final_prob = [[0]*26 for _ in range(self.empty_sites)]\n",
    "        self.fivegram_adder(clean_word)\n",
    "        self.fourgram_adder(clean_word)\n",
    "        self.threegram_adder(clean_word)\n",
    "        self.twogram_adder(clean_word)\n",
    "        self.unigram_adder(clean_word)\n",
    "        self.final_prob = np.array(self.final_prob)\n",
    "        self.final_prob = self.final_prob / self.final_prob.sum(axis = 1, keepdims=True)\n",
    "        self.final_prob = self.final_prob.sum(axis = 0)\n",
    "        \n",
    "        guess_letter = '!'\n",
    "\n",
    "        temp, letter = 0, 0\n",
    "        for letter in self.alphabets:\n",
    "            if temp < self.final_prob[ord(letter) - ord('a')] and letter not in self.guessed_letters:\n",
    "                temp = self.final_prob[ord(letter) - ord('a')]\n",
    "                guess_letter = letter\n",
    "        return guess_letter\n",
    "        \n",
    "    def frequency_calc(self, dictionary):\n",
    "        # returns unigram / twogram / threegram / fourgram / fivegram frequencies\n",
    "        \n",
    "        unigram   = collections.defaultdict(lambda: collections.defaultdict(lambda : collections.defaultdict(int)))\n",
    "        twogram   = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "        threegram = collections.defaultdict(lambda: collections.defaultdict(lambda : collections.defaultdict(int)))\n",
    "        fourgram  = collections.defaultdict(lambda: collections.defaultdict(lambda : collections.defaultdict(lambda : collections.defaultdict(int))))\n",
    "        fivegram  = collections.defaultdict(lambda: collections.defaultdict(lambda : collections.defaultdict(lambda : collections.defaultdict(lambda : collections.defaultdict(int)))))\n",
    "            # unigram[wordlength][letterlocation][letter] = ...\n",
    "            # twogram[letter_1][letter_2] = ...\n",
    "            # threegram[letter_1][letter_2][letter_3] = ...\n",
    "            # fourgram[letter_1][letter_2][letter_3][letter_4] = ...\n",
    "            # fivegram[letter_1][letter_2][letter_3][letter_4][letter_5] = ...\n",
    "        \n",
    "        for word in dictionary:\n",
    "            current_length = len(word)\n",
    "            if current_length == 1:\n",
    "                unigram[current_length][0][word[0]] += 1\n",
    "            elif current_length == 2:\n",
    "                unigram[current_length][0][word[0]] += 1\n",
    "                unigram[current_length][1][word[1]] += 1\n",
    "                twogram[word[0]][word[1]] += 1\n",
    "            elif current_length == 3:\n",
    "                unigram[current_length][0][word[0]] += 1\n",
    "                unigram[current_length][1][word[1]] += 1\n",
    "                unigram[current_length][2][word[2]] += 1\n",
    "                twogram[word[0]][word[1]] += 1\n",
    "                twogram[word[1]][word[2]] += 1\n",
    "                threegram[word[0]][word[1]][word[2]] += 1\n",
    "            elif current_length == 4:\n",
    "                unigram[current_length][0][word[0]] += 1\n",
    "                unigram[current_length][1][word[1]] += 1\n",
    "                unigram[current_length][2][word[2]] += 1\n",
    "                unigram[current_length][3][word[3]] += 1\n",
    "                twogram[word[0]][word[1]] += 1\n",
    "                twogram[word[1]][word[2]] += 1\n",
    "                twogram[word[2]][word[3]] += 1\n",
    "                threegram[word[0]][word[1]][word[2]] += 1\n",
    "                threegram[word[1]][word[2]][word[3]] += 1\n",
    "                fourgram[word[0]][word[1]][word[2]][word[3]] += 1\n",
    "            \n",
    "            else : # current_length >= 5\n",
    "                for i in range(current_length - 4):\n",
    "                    unigram[current_length][i][word[i]] += 1\n",
    "                    twogram[word[i]][word[i+1]] += 1\n",
    "                    threegram[word[i]][word[i+1]][word[i+2]] += 1\n",
    "                    fourgram[word[i]][word[i+1]][word[i+2]][word[i+3]] += 1\n",
    "                    fivegram[word[i]][word[i+1]][word[i+2]][word[i+3]][word[i+4]] += 1\n",
    "                # corner cases\n",
    "                unigram[current_length][current_length-1][word[-1]] += 1\n",
    "                unigram[current_length][current_length-2][word[-2]] += 1\n",
    "                unigram[current_length][current_length-3][word[-3]] += 1\n",
    "                unigram[current_length][current_length-4][word[-4]] += 1\n",
    "                twogram[word[-2]][word[-1]] += 1\n",
    "                twogram[word[-3]][word[-2]] += 1\n",
    "                twogram[word[-4]][word[-3]] += 1\n",
    "                threegram[word[-3]][word[-2]][word[-1]] += 1\n",
    "                threegram[word[-4]][word[-3]][word[-2]] += 1\n",
    "                fourgram[word[-4]][word[-3]][word[-2]][word[-1]] += 1\n",
    "                \n",
    "        return unigram, twogram, threegram, fourgram, fivegram\n",
    "\n",
    "    def fivegram_adder(self, word):\n",
    "        '''\n",
    "        counts all possible fivegram_frequency and directly add to self.final_prob\n",
    "        returns nothing\n",
    "        '''\n",
    "        wordshape, count = [0]*len(word), 0\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == \".\": \n",
    "                wordshape[i] = count\n",
    "                count += 1\n",
    "                \n",
    "        fiveprob, count = [[0]*26 for _ in range(self.empty_sites)], [0]*self.empty_sites\n",
    "        for i in range(len(word) - 4):\n",
    "            if word[i] != \".\" and word[i+1] != \".\" and word[i+2] != \".\" and word[i+3] != \".\" and word[i+4] == \".\":\n",
    "                # ****. form\n",
    "                for letter in self.alphabets:\n",
    "                    fiveprob[wordshape[i+4]][ord(letter) - ord('a')] += self.fivegram_frequency[word[i]][word[i+1]][word[i+2]][word[i+3]][letter]\n",
    "                    count[wordshape[i+4]] += self.fivegram_frequency[word[i]][word[i+1]][word[i+2]][word[i+3]][letter]\n",
    "            elif word[i] != \".\" and word[i+1] != \".\" and word[i+2] != \".\" and word[i+3] == \".\" and word[i+4] != \".\":\n",
    "                # ***.* form\n",
    "                for letter in self.alphabets:\n",
    "                    fiveprob[wordshape[i+3]][ord(letter) - ord('a')] += self.fivegram_frequency[word[i]][word[i+1]][word[i+2]][letter][word[i+4]]\n",
    "                    count[wordshape[i+3]] += self.fivegram_frequency[word[i]][word[i+1]][word[i+2]][letter][word[i+4]]\n",
    "            elif word[i] != \".\" and word[i+1] != \".\" and word[i+2] == \".\" and word[i+3] != \".\" and word[i+4] != \".\":\n",
    "                # **.** form\n",
    "                for letter in self.alphabets:\n",
    "                    fiveprob[wordshape[i+2]][ord(letter) - ord('a')] += self.fivegram_frequency[word[i]][word[i+1]][letter][word[i+3]][word[i+4]]\n",
    "                    count[wordshape[i+2]] += self.fivegram_frequency[word[i]][word[i+1]][letter][word[i+3]][word[i+4]]  \n",
    "            elif word[i] != \".\" and word[i+1] == \".\" and word[i+2] != \".\" and word[i+3] != \".\" and word[i+4] != \".\":\n",
    "                # *.*** form\n",
    "                for letter in self.alphabets:\n",
    "                    fiveprob[wordshape[i+1]][ord(letter) - ord('a')] += self.fivegram_frequency[word[i]][letter][word[i+2]][word[i+3]][word[i+4]]\n",
    "                    count[wordshape[i+1]] += self.fivegram_frequency[word[i]][letter][word[i+2]][word[i+3]][word[i+4]]\n",
    "            elif word[i] == \".\" and word[i+1] != \".\" and word[i+2] != \".\" and word[i+3] != \".\" and word[i+4] != \".\":\n",
    "                # .**** form\n",
    "                for letter in self.alphabets:\n",
    "                    fiveprob[wordshape[i]][ord(letter) - ord('a')] += self.fivegram_frequency[letter][word[i+1]][word[i+2]][word[i+3]][word[i+4]]\n",
    "                    count[wordshape[i]] += self.fivegram_frequency[letter][word[i+1]][word[i+2]][word[i+3]][word[i+4]]            \n",
    "        \n",
    "        # normalization\n",
    "        for i in range(self.empty_sites):\n",
    "            if count[i] != 0:\n",
    "                for letter in self.alphabets:\n",
    "                    self.final_prob[i][ord(letter) - ord('a')] += self.weight[4] * 1.0* fiveprob[i][ord(letter) - ord('a')]/count[i]\n",
    "\n",
    "    def fourgram_adder(self, word):\n",
    "        '''\n",
    "        counts all possible fourgram_frequency and directly add to self.final_prob\n",
    "        returns nothing\n",
    "        '''\n",
    "        wordshape, count = [0]*len(word), 0\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == \".\": \n",
    "                wordshape[i] = count\n",
    "                count += 1\n",
    "        \n",
    "        fourprob, count = [[0]*26 for _ in range(self.empty_sites)], [0]*self.empty_sites\n",
    "        for i in range(len(word) - 3):\n",
    "            if word[i] != \".\" and word[i+1] != \".\" and word[i+2] != \".\" and word[i+3] == \".\":       # ***. form\n",
    "                for letter in self.alphabets:\n",
    "                    fourprob[wordshape[i+3]][ord(letter) - ord('a')] += self.fourgram_frequency[word[i]][word[i+1]][word[i+2]][letter]\n",
    "                    count[wordshape[i+3]] += self.fourgram_frequency[word[i]][word[i+1]][word[i+2]][letter]\n",
    "            elif word[i] != \".\" and word[i+1] != \".\" and word[i+2] == \".\" and word[i+3] != \".\":     # **.* form\n",
    "                for letter in self.alphabets:\n",
    "                    fourprob[wordshape[i+2]][ord(letter) - ord('a')] += self.fourgram_frequency[word[i]][word[i+1]][letter][word[i+3]]\n",
    "                    count[wordshape[i+2]] += self.fourgram_frequency[word[i]][word[i+1]][letter][word[i+3]]\n",
    "            elif word[i] != \".\" and word[i+1] == \".\" and word[i+2] != \".\" and word[i+3] != \".\":     # *.** form\n",
    "                for letter in self.alphabets:\n",
    "                    fourprob[wordshape[i+1]][ord(letter) - ord('a')] += self.fourgram_frequency[word[i]][letter][word[i+2]][word[i+3]]\n",
    "                    count[wordshape[i+1]] += self.fourgram_frequency[word[i]][letter][word[i+2]][word[i+3]]\n",
    "            elif word[i] == \".\" and word[i+1] != \".\" and word[i+2] != \".\" and word[i+3] != \".\":     # .*** form\n",
    "                for letter in self.alphabets:\n",
    "                    fourprob[wordshape[i]][ord(letter) - ord('a')] += self.fourgram_frequency[letter][word[i+1]][word[i+2]][word[i+3]]\n",
    "                    count[wordshape[i]] += self.fourgram_frequency[letter][word[i+1]][word[i+2]][word[i+3]]\n",
    "        \n",
    "        # normalization\n",
    "        for i in range(self.empty_sites):\n",
    "            if count[i] != 0:\n",
    "                for letter in self.alphabets:\n",
    "                    self.final_prob[i][ord(letter) - ord('a')] += self.weight[3] * 1.0 * fourprob[i][ord(letter) - ord('a')]/count[i]\n",
    "\n",
    "    def threegram_adder(self, word):\n",
    "        '''\n",
    "        counts all possible threegram_frequency and directly add to self.final_prob\n",
    "        returns nothing\n",
    "        '''\n",
    "        wordshape, count = [0]*len(word), 0\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == \".\": \n",
    "                wordshape[i] = count\n",
    "                count += 1\n",
    "        \n",
    "        threeprob, count = [[0]*26 for _ in range(self.empty_sites)], [0]*self.empty_sites\n",
    "        for i in range(len(word) - 2):\n",
    "            if word[i] != \".\" and word[i+1] != \".\"  and word[i+2] == \".\":       # **. form\n",
    "                for letter in self.alphabets:\n",
    "                    threeprob[wordshape[i+2]][ord(letter) - ord('a')] += self.threegram_frequency[word[i]][word[i+1]][letter]\n",
    "                    count[wordshape[i+2]] += self.threegram_frequency[word[i]][word[i+1]][letter]\n",
    "            elif word[i] != \".\" and word[i+1] == \".\"  and word[i+2] != \".\":     # *.* form\n",
    "                for letter in self.alphabets:\n",
    "                    threeprob[wordshape[i+1]][ord(letter) - ord('a')] += self.threegram_frequency[word[i]][letter][word[i+2]]\n",
    "                    count[wordshape[i+1]] += self.threegram_frequency[word[i]][letter][word[i+2]]\n",
    "            elif word[i] == \".\" and word[i+1] != \".\"  and word[i+2] != \".\":     # .** form\n",
    "                for letter in self.alphabets:\n",
    "                    threeprob[wordshape[i]][ord(letter) - ord('a')] += self.threegram_frequency[letter][word[i+1]][word[i+2]]\n",
    "                    count[wordshape[i]] += self.threegram_frequency[letter][word[i+1]][word[i+2]]\n",
    "        \n",
    "        # normalization\n",
    "        for i in range(self.empty_sites):\n",
    "            if count[i] != 0:\n",
    "                for letter in self.alphabets:\n",
    "                    self.final_prob[i][ord(letter) - ord('a')] += self.weight[2] * 1.0 * threeprob[i][ord(letter) - ord('a')]/count[i]\n",
    "            \n",
    "    def twogram_adder(self, word):\n",
    "        '''\n",
    "        counts all possible twogram_frequency and directly add to self.final_prob\n",
    "        returns nothing\n",
    "        '''\n",
    "        wordshape, count = [0]*len(word), 0\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == \".\": \n",
    "                wordshape[i] = count\n",
    "                count += 1\n",
    "        \n",
    "        twoprob, count = [[0]*26 for _ in range(self.empty_sites)], [0]*self.empty_sites\n",
    "        for i in range(len(word) - 1):\n",
    "            if word[i] != \".\" and word[i+1] == \".\":       # *. form\n",
    "                for letter in self.alphabets:\n",
    "                    twoprob[wordshape[i+1]][ord(letter)-ord('a')] += self.twogram_frequency[word[i]][letter]\n",
    "                    count[wordshape[i+1]] += self.twogram_frequency[word[i]][letter]\n",
    "            elif word[i] == \".\" and word[i+1] != \".\":     # .* form\n",
    "                for letter in self.alphabets:\n",
    "                    twoprob[wordshape[i]][ord(letter)-ord('a')] += self.twogram_frequency[letter][word[i+1]]\n",
    "                    count[wordshape[i]] += self.twogram_frequency[letter][word[i+1]]\n",
    "        \n",
    "        # normalization\n",
    "        for i in range(self.empty_sites):\n",
    "            if count[i] != 0:\n",
    "                for letter in self.alphabets:\n",
    "                    self.final_prob[i][ord(letter) - ord('a')] += self.weight[1] * 1.0 * twoprob[i][ord(letter) - ord('a')]/count[i]\n",
    "    \n",
    "    def unigram_adder(self, word):\n",
    "        '''\n",
    "        counts all possible onegram_frequency and directly add to self.final_prob\n",
    "        returns nothing\n",
    "        '''\n",
    "        wordshape, count = [0]*len(word), 0\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == \".\": \n",
    "                wordshape[i] = count\n",
    "                count += 1\n",
    "                \n",
    "        oneprob, count = [[0]*26 for _ in range(self.empty_sites)], [0]*self.empty_sites\n",
    "        for i in range(len(word)):\n",
    "            if word[i] == \".\":\n",
    "                if i == 0 or i == len(word) - 1:        # empty_sites for first/last letters\n",
    "                    for letter in self.alphabets:\n",
    "                        oneprob[wordshape[i]][ord(letter) - ord('a')] += self.unigram_frequency[len(word)][i][letter]\n",
    "                        count[wordshape[i]] += self.unigram_frequency[len(word)][i][letter]\n",
    "                else:                                   # other empty_sites. apply the usual frequency information.\n",
    "                    for letter in self.alphabets:\n",
    "                        for j in range(len(word)):\n",
    "                            oneprob[wordshape[i]][ord(letter) - ord('a')] += self.unigram_frequency[len(word)][j][letter]\n",
    "                            count[wordshape[i]] += self.unigram_frequency[len(word)][j][letter]\n",
    "        \n",
    "        # normalization\n",
    "        for i in range(self.empty_sites):\n",
    "            if count[i] != 0:\n",
    "                for letter in self.alphabets:\n",
    "                    self.final_prob[i][ord(letter) - ord('a')] += self.weight[0] * 1.0 * oneprob[i][ord(letter) - ord('a')]/count[i]\n",
    "\n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T20:17:50.611690600Z",
     "start_time": "2024-09-10T20:17:50.596043900Z"
    }
   },
   "id": "5f0c1d6656fac2ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Trials and Ideas"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9376ba732a785fea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1. (1-gram) + (2-gram) + (3-gram) + (4-gram) only\n",
    "\n",
    "This is the very first model that I have tried. As I couldn't easily imagine 5 letter prefixes/suffixes, I tried only\n",
    " upto 4-grams, without 5-gram information. I have tried 100 examples on the server, and I got 45 out of 100. After I \n",
    " tried models with 5-grams and realized there are meaningful 5-grams, I decided not to work on this model. All models\n",
    "  with 5-gram I have tried exceeds 50% win rate, with more than 500 games.\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10a10fe1585ddd4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2. soft n-gram\n",
    "\n",
    "3-gram does not work if there are two or more unknown letters. For example, if we want to guess a letter from \"ab_ent\", \n",
    "the 3-gram model calculates frequency over \"aba\", \"abb\", ..., \"abz\", \"bae\", \"bbe\", ..., \"bze\", \"aen\", \"ben\", ..., \n",
    "\"zen\" but the model cannot calculate frequencies from \"a__ent\". I thought about generalizing this n-gram idea and tried\n",
    " to calculate frequency over \"aaa\" ... \"azz\" (total $26^2$ ways) and \"aae\" ... \"zze\" (total $26^2$ ways). However \n",
    " this didn't work well, and I think it is because the weight for 3-gram is larger than 1-gram, and random sequences like \"aaa\" or \"azz\" \n",
    "weaken the information from 1-gram. I decided not to apply this n-gram idea with multiple unknown characters."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cf684beb8c56eea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3. wordlength\n",
    "\n",
    "My model guesses the first letter according to the frequency over the words with same length, not over all words from\n",
    " the dictionary. This is because the most frequent letter differs by the length of the given word(<a href = http://www.datagenetics.com/blog/april12012/>http://www.datagenetics.com/blog/april12012/</a>).\n",
    "Hence I decided to also consider the length of the given word and keep restricted for unigram_frequency during the \n",
    "entire guess. We need to rigorously check with conditional probability argument to verify, but empirically this gives\n",
    " better prediction score.\n",
    "\n",
    "Some people suggests to apply the word length information to 2-gram so that we can selectively use 2-grams too, but \n",
    "this turned out to lower the prediction score, so I decided not to.\n",
    "<br>\n",
    "\n",
    "Similarly we can compare with the model with unigram not using word length information, but this also lowers the \n",
    "prediction score."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee244aafed4d24c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.4. dictionary reselection\n",
    "\n",
    "Suppose the model guess a letter \"e\" and it is wrong. This also includes information that it does not have the letter,\n",
    " and we can calibrate \"self.current_dictionary\" by excluding all words with \"e\". We might try to exclude words \n",
    " every time we incorrectly guesses, but this decreases the program speed, and (surprisingly) the prediction score. I \n",
    " think this happens because each wrong guess excludes so many words and the remaining frequency information becomes too \n",
    " sparser. At some point we need to adjust the dictionary for better prediction score, but I decided not to perform the calibration \n",
    " process at the early stages. The model only recalibrates when it's previous guess is wrong and there are only 1 or 2 \n",
    " future wrong guesses are remaining."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cc8876dafc60558"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.5. Consonant sequence analysis\n",
    "\n",
    "For simplicity, I would like to write C and V for consonants and vowels.\n",
    "<br>\n",
    "\n",
    "In a linguistic sense, consecutive consonants are hard to pronounce. This makes a word with consecutive consonants \n",
    "are uncommon. For example, Typical 4-letter words have a form of CVCV either CVVC, which leads \"CC\" configuration is \n",
    "relatively less common. Of course there are combined consonants such as \"ch\" and \"sh\", which are even a single letter\n",
    " on Phonetic Alphabets, and there is a possibility that the given word can be split into two words which finishes \n",
    " with CC and starts with CC, such as \"backspace\"(CCCC), or even \"worthwhile\"(CCCCC).\n",
    "<br>\n",
    "\n",
    "Sometimes 'y' works as a vowel (\"tiny\", \"flurry\") so I exclude 'y' from consonants, and tried to calculate \n",
    "frequencies of CCCCC and CCCCCC. There are 540 words with CCCCC and 43 words with CCCCCC out of 227300 words. I \n",
    "modified the model less likely to choose a letter which generates CCCCC, and even exclude CCCCCC. However, this does \n",
    "not enhance model's prediction score(the score was not statistically significantly better than the best among the \n",
    "others) while it significantly slows down the program. Hence I decided not to implement this idea to the model. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92d8f0cd2a9af6a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.6. padding at the beginning / ending\n",
    "\n",
    "One of the common defect of n-gram model is that does not consider the location of each letter. Consider a word \n",
    "\"_bsent\", which is \"absent\". 2-gram will give you the ratio of (number of \"_b\"s)/(sum of the numbers of \"*b\"s), \n",
    "where * is a wildcard(can be any single letter). This calculation is not utilizing the fact that this blank is at \n",
    "the front. Heuristically we can guess 'a' with this fact, so it is possible to add some paddings on each\n",
    "words and also calculate the frequency over those.\n",
    "<br>\n",
    "  \n",
    "For example, for 2-grams, we can modify the word \"absent\" to \"{{absent{{\", and add \"{a\", \"ab\", \"bs\", \"se\", \"en\", \n",
    "\"nt\", \"t{\" to the frequency information. This becomes particularly helpful in case of \"_bsent\" -> \"absent\", by using \n",
    "two 2-grams, \"{_\" and \"_b\".\n",
    "<br>\n",
    "\n",
    "However, this turned out to give too biased results at the early stages because only the first letter and the last \n",
    "letter can utilize 2-gram information while all the other letters cannot. I decided to implement this first/last \n",
    "letter information in another way, and to this end I devised the below, 3.1.7."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f552b30e981e63a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.7. probability calculating each sites vs probability calculating only over alphabets\n",
    "\n",
    "I thought the first/last letter frequency over all letters must be different from the usual letter frequency, \n",
    "so I decided to calculate those separately and replace usual 1-gram results to these. However this idea requires me \n",
    "to calculate probability of each alphabet on every site, and this requires one more step to think of because the \n",
    "model only guesses one letter each time, not specifying possible sites with the letter.\n",
    "<br>\n",
    "\n",
    "This takes longer calculation time, but I decided to consider (26, self.empty_sites) size of probability matrix, not \n",
    "just (26, 1). After I finish collecting information from 5-gram to 1-gram, the model normalizes probability on each \n",
    "empty site, and sum them up by each letter. The model chooses the letter with the largest probability sum from each \n",
    "empty site. \n",
    "\n",
    "Even though this takes longer time, the model showed better performance over practice set, around 58%."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a0d181307d50cc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eef4dedc37e9f00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "I submitted the final test at 2/20 afternoon. Total calculation took 4 hours and 38 minutes, and the result was 568 \n",
    "correct word guesses out of 1000 trials."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3c84f6e9634691"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
